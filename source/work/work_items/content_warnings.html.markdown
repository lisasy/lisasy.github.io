---

title: content_warnings
subtitle: "Using Content Warnings to Protect People From Unwanted Content on Facebook"
date: 2018-08-21 17:04 UTC
blog: work
sourceline: Product Design Lead at Facebook (2015-2017)
role: Product Design Lead
header_img: "/assets/images/work/fb_content_warnings/header_bg.png"
tags: 
layout: "work_project"

---

<div class="work_section overview col_full">
  <div class="col">
    <p>
      From 2015-2017, I led the product design to develop content warnings on
      Facebook to help people deal with unexpected, sensitive content that they
      encounter on Facebook. I understood, strategized, and collaborated with
      an interdisciplinary team to weigh different considerations and
      capacities from Facebook’s Policy and Machine-Learning team, to our
      international research team, with implementing an experience that would
      best solve our community’s needs. I also presented the strategy and
      design directly to Mark Zuckerberg to receive his feedback and guidance.
      <strong>Today, the content warning is accessible on all platforms (web,
      iOS, Android) to 100% of Facebook’s global market, viewable most
      popularly on the News Feed. <u>It resulted in higher sentiment and decreased
      reporting of negative content on Facebook.</u></strong>
    </p>
  </div>
</div>


<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/project_overview.png" />
    </figure>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
    <h5 class="work_section_title">
      Understanding the Problem
    </h5>
    <h2>
      When people go onto Facebook and unexpectedly encounter sensitive
      content, it is a jarring experience that bothers them.
		</h2>
		<p>
      Sometimes, seeing this graphic content without any warnings triggers past
      trauma. Other times, it results in pure embarassment if they’re browsing
      Facebook in public. <strong>In the inevitable situation where Facebook
      can’t proactively remove potentially offensive or suggestive content, how
      can we offer a remediary experience?</strong>
    </p>
		<p>
      Employing the jobs-to-be-done approach to UX design and research, I
      sought to understand what the community did <strong>today</strong> to
      deal with the issues they encountered. Working with my team’s data
      scientist, we noted that the high volume of reported content indicated
      that people were emboldened to take action on such content. Although
      about 2.9% of content viewed on Facebook are sexually suggestive,
      non-policy-violating, only about 0.07% of sexual/nude content viewed on
      Facebook violate its policy.
		</p>
		<p>
      If not all of this content can be removed, <strong>what more can we
      do?</strong>
		</p>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
    <h5 class="work_section_title">
      The Crux
    </h5>
    <h2>
      When people share content on Facebook, it is an expression of their
      values, identity, and personhood online. Moderating content on Facebook
      is not only about aiding the needs of the viewer, but also respecting the
      freedom of expression for the poster.
    </h2>
  </div>
</div>

<hr>

<div class="work_section col_full">
  <div class="col">
    <h2>
      Approaching a Strategy
    </h2>
		<p>
      Given the complexity of our different users’ needs, what considerations must we
      acknowledge if we implemented warning screens on Facebook? We asked ourselves: 
		</p>

		<p>
			<ol>
				<li>
          How do we resolve the pain points of the person seeing unwanted
          content, with the freedom of expression we grant to person who
          posted such content in the first place?
				</li>
				<li>
          How do we give people control over how “strict” the content warning should be?
          And how we give them control when they think that Facebook has incorrectly
          covered up innocuous content?
				</li>
				<li>
          How would we design and implement an adaptable, modular product experience that
          could apply to various “story” formats on Facebook, a product on which numerous
          product teams drive their unique product direction (written post, photos,
          videos, and other miscellaneous formats)?
				</li>
			</ol>
		</p>

  </div>
</div>

<hr>

<div class="work_section col_full highlight align_center">
  <div class="col">
    <h2>
      Defining Our Principles
    </h2>
    <p class="special_paragraph">
      In order to guide the team to make decisions around these complex
      considerations, I collaborated with my team's content strategist to
      brainstorm and design product principles:
    </p>

    <article class="list_item">
      <div class="list_counter">
        <h5 class="list_counter_value">
          01
        </h5>
      </div>
      <div class="list_content">
        <h4 class="list_content_title">
          Clarity
        </h4>
        <p class="list_content_description">
          Communicate to users that this content is concealed, and why.
        </p>
      </div>
    </article>

    <article class="list_item">
      <div class="list_counter">
        <h5 class="list_counter_value">
          02
        </h5>
      </div>
      <div class="list_content">
        <h4 class="list_content_title">
          Control
        </h4>
        <p class="list_content_description">
          Give affordance for users to cover and uncover this photo/video.
        </p>
      </div>
    </article>

    <article class="list_item">
      <div class="list_counter">
        <h5 class="list_counter_value">
          03
        </h5>
      </div>
      <div class="list_content">
        <h4 class="list_content_title">
          Voice
        </h4>
        <p class="list_content_description">
          If the content warning is connected to a user setting, then give
          users the option to flag this content warning as a false positive
          or to edit their user settings.
        </p>
      </div>
    </article>
  </div>
  <figure>
    <img src="/assets/images/work/fb_content_warnings/principles.png" />
  </figure>
</div>

<div class="work_section col_full">
  <div class="col">
    <h5 class="work_section_title">
      Initial Explorations
    </h5>
    <h2>
      What could content warnings look like?
    </h2>
		<p>
      Both the aforementioned considerations and the product principles provide
      some constraints for the next phase, which is to broadly explore the
      possibilities of what how this content warning can take shape. During
      this initial phase, it is important to explore widely and divergent in
      order to expose any limitations of our technical capacity, highlight any
      philosophical questions we'd need to address, and uncover any "edge
      cases" we may not had been thinking of previously.
		</p>
  </div>
</div>

<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/breadth.png" />
    </figure>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
		<p>
      One stream of explorations I delved into was understanding how much
      content the content warning should cover.
		</p>
		<p>
      Since one of the primary considerations was to balance alleviating the
      painful experience for the viewer, with the freedom expression of the
      content creator, I focused my design explorations with this in mind.
		</p>
  </div>
</div>

<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/cw_variation_1.png" />
    </figure>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
    <h2>
      Presenting Our Point of View to Mark Zuckerberg
    </h2>
		<p>
      After many continued explorations and feedback sessions from my peers,
      who included product designers, researchers, and engineers from my Safety
      and Security team, as well as those from the News Feed team, I refined
      the concept to the video shown below: a content warning blurring only the
      related visual content (photo or video), that lets the user know why it's
      covered, what they can do if they continue to see unwanted content, and a
      way to let Facebook know if it accidentally covered innocuous content.
		</p>
		<h4>
      Given the intense sensitivity of what my team was conceiving and its
      possible and grave implications for the community and for Facebook, this
      concept went through ample review at the executive level. With my core
      team in attendance, I presented this design concept directly to Mark
      Zuckerberg to receive his feedback around the design execution, the
      technical execution, and the unanswered questions we needed a gut check
      from him on. 
		</h4>
  </div>
</div>

<div class="work_section no_margin gif col_full">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/cw_version_1.gif" />
      <figcaption>
        Early video demo of a prototype concept
      </figcaption>
    </figure>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
		<p>
      Mark encouraged our team to continue to iterate on our messaging and
      presentation of the content warning overlay, as well as its settings, so
      that it did not convey a judgement around someone's personal morals
      around content they don't want to see. 
		</p>
  </div>
</div>

<hr>

<div class="work_section col_full">
  <div class="col">
    <h2>
      Iterating After Mark Zuckerberg's Feedback
    </h2>
		<p>
      After the review with Mark, I continued to iterate on my content
      warnings and the settings. Given the expanding scope of the original
      project, I drove the team to differentiate the two specific workstreams
      we were beginning to work within. Even though they were closely related,
      they needed to be distinct so that we can be focused in our
      problem-solving and investigation.
		</p>
		<p>
      With <strong>content warnings</strong>, I iterated upon developing a
      modular, flexible content warning that can support the right messaging
      for our community. I worked closely with the content strategist and with
      policy to gather their feedback and needs.
		</p>
  </div>
</div>

<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/exploration_warnings.png" />
    </figure>
  </div>
</div>

<hr>

<div class="work_section col_full">
  <div class="col">
		<p>
    With <strong>Settings</strong>, I continued to explore what a
    community-moderated content preference settings might look like on
    Facebook. Gathering the known requirements from the Policy and our
    Engineering team, we explored and focused in on three possible directions
    that we needed to validate and understand better.
		</p>
		<p>
      How do people feel about their "community" helping moderate content that
      they see on Facebook? What do people even consider to be flagrant
      content? We had a lot of hypotheses but not enough clear evidence,
      especially at a global scale. Thus, we set out on a research trip.
		</p>
  </div>
</div>

<div class="work_section no_margin col col_2">
  <div class="work_section_body_container">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/exploration_settings_1.png" />
    </figure>
  </div>
</div>

<div class="work_section col_full">
  <div class="col">
    <h5 class="work_section_title">
      User Research
    </h5>
    <h2>
      We went to India, Germany, Indonesia, and Mexico to better understand how
      people think about their community's role in determining how content
      moderation might work on Facebook.
    </h2>
    <p> 
      There were numerous questions and hypotheses we sought to better
      understand during this trip. The key themes were:

      <ul class="styled">
        <li>
          <strong>
            What is Facebook's role in handling content described as sexually
            suggestive, hate speech, violence/gore, et cetera?
          </strong>
            This necessitated us to understand how the participants categorize
            and describe such content.
        </li>
        <li>
          <strong>
            How much do they trust their community's ability to help moderate
            this content on Facebook?
          </strong>
            This required better understanding how they describe "community":
            is it defined by shared interests? Demographics? Location?
            it were a location, how might that work on a global scale?
        </li>
        <li>
          <strong>
            Which interaction design patterns best match their expectations
            when dealing with such content?
          </strong>
        </li>
      </ul>
    </p>
  </div>
</div>

<hr>

<div class="work_section col_6 align_center">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/research_indonesia.jpg" />
    </figure>
  </div>
  <div class="col">
    <p>
      During to the complexities of these questions, we answered them through a
      variety of methods: qualitative (interviews, categorical card-sorting,
      focus group discussions), and quantitative (usability testing).
    </p>
    <p>
      For the individual research sessions, we talked with each participant for
      about 2 hours. In total, we spoke directly with 200 people spanning four
      continents.
    </p>
  </div>
</div>

<hr>

<div class="work_section col_full">
  <div class="col">
    <h2>Key Takeaways</h2>
    <p> 
      After two intensive weeks of international research, we came away with
      these key takeaways:
    </p>
    <p>
      <ul class="styled">
        <li>
          <strong>Community is subjective...</strong><br>
          Across the diversity of race, religious affiliation, social class,
          sexual orientation, gender identity, geographic location, interests,
          countries, and languages of people we talked to, there are ample
          nuances and no definitive consensus of a definition of a "community".
          Simply put, a community is a group of people, but the definition of
          the group changes meaning and context during different circumstances.
        </li>
        <li>
          <strong>... so is what people consider flagrant on
          Facebook.</strong><br> Not only do people define their "community"
          subjectively, but so too is what they consider to be flagrant on
          Facebook. This affirmed our primary aim of giving people control to
          see what they want to see on Facebook, validating our content warning
          feature.
        </li>
        <li>
          <strong>General UX/UI Feedback/Content</strong><br> After testing 3
          variations of the content warning settings, we found that people
          wanted that the words they'd see to describe this content warning
          matters just as much as the UX/UI showcasing it. Following our
          direction, here were iterations I came up with afterwards.
        </li>
      </ul>
    </p>
  </div>
</div>

<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img
      src="/assets/images/work/fb_content_warnings/exploration_settings_2.png"
      />
    </figure>
  </div>
</div>

<hr>

<div class="work_section col_full">
  <div class="col">
    <h2>
      The final push
    </h2>
    <p> 
      During the next few weeks after the research session, I worked with my
      team to execute on our launch plan for our MVP. Taking into consideration
      the: 1) deadline, 2) legal and technical possibilities, and 3) necessary
      features to solve our core user problem, we worked through all of the
      implementation details, from the UI/UX design, to meeting policy and
      legal requirements, to working closely with our marketing and
      communication partnerships.
    </p>
    <h2>
      <strong>In April 2017, the content warning launched to all platforms
      (web, iOS, Android) to 100% of Facebook’s global market, viewable most
      popularly on the News Feed.</strong>
    </h2>
  </div>
</div>

<div class="work_section no_margin col_full_stretch">
  <div class="col">
    <figure>
      <img src="/assets/images/work/fb_content_warnings/project_overview.png" />
    </figure>
  </div>
</div>
